{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a90301e5",
   "metadata": {},
   "source": [
    "# Implementing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "from Bandit import Bandit\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb8b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5efb3b",
   "metadata": {},
   "source": [
    "# Defining the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ed9126",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 20000\n",
    "bandit_rewards_list = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20fe60",
   "metadata": {},
   "source": [
    "# Class implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyBandit(Bandit):\n",
    "    \"\"\"\n",
    "    class EpsilonGreedyBandit. \n",
    "    Represents a epsilon greedy bandit with the following attributes and methods.\n",
    "    \n",
    "    Attributes:\n",
    "        reward (float): The true reward of the bandit.\n",
    "    Methods:\n",
    "        pull(): Pulls a value for the bandit.\n",
    "        update(x: float): Updates the estimated reward of the bandit.\n",
    "        plot1(): Plots the learning process of the bandits.\n",
    "        experiment(bandit_rewards: list, num_trials: int, path_to_save: str, to_save: bool): Run the experiment with given bandits, number of trials and whether to save the results or not.\n",
    "        report(path_to_save: str, to_save: bool): Prints the report of the experiment.\n",
    "        create_data(): Creates the dataframe for saving\n",
    "        update_epsilon(): Updates the epsilon with rate 1/t.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, reward):\n",
    "        \"\"\"\n",
    "        Initilizes a new instance of EpsilonGreedyBandit class with the following arguments:\n",
    "        \n",
    "        Args:\n",
    "            reward (float): The true reward of the bandit. \n",
    "        \n",
    "        \"\"\"\n",
    "        self.reward = reward     #True reward\n",
    "        self.reward_estimate = 0.    #Reward estimate\n",
    "        self.N = 0    #Iteration count\n",
    "        self.learning_process = []    #Learning process list\n",
    "        self.bandits = None    #The bandits list\n",
    "        self.epsilon = 1    #Starting epsilon\n",
    "        \n",
    "    def pull(self):\n",
    "        \"\"\"\n",
    "        Pulls a value for the bandit based on the true reward (the value is pulled from normal distribution with mean = \"true bandit reward\" and 1 std).\n",
    "        \n",
    "        \"\"\"\n",
    "        return np.random.normal(self.reward, 1)     #Pull a reward with mean \"reward\" and std 1\n",
    "    \n",
    "    def update(self, x):\n",
    "        \"\"\"\n",
    "        Updates the estimated reward for the bandit based on the new, pulled value.\n",
    "        \n",
    "        Args:\n",
    "            x (float): The new value pulled for the bandit, the value to update for the estimated reward.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.N += 1    #Add 1 to the iteration count\n",
    "        self.reward_estimate = ((self.N - 1)*self.reward_estimate + x) / self.N    #Update the reward estimate\n",
    "        self.learning_process.append(self.reward_estimate)    #Add the updated value to the reward list\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'A bandit with actual reward: {self.reward} and estimated reward {self.reward_estimate}'\n",
    "    \n",
    "    def plot1(self):\n",
    "        \"\"\"\n",
    "        Plots the learning curves after the experiment for the existing bandits. Creates a subplot of all the bandits learning curves.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.bandits is None:    #If no experiment was done, raise exception\n",
    "            raise Exception('Run the experiment first')\n",
    "        fig, axs = plt.subplots(2, 2, figsize = (10, 8))    #Create the subplots\n",
    "        plt.suptitle('The learning curves of all 4 bandits', size=15)    #Add the head title for all subplots\n",
    "        for index, bandit in enumerate(self.bandits):    #For each bandit plot the result\n",
    "            x_index = 1 if index>=2 else 0 \n",
    "            y_index = index%2\n",
    "            axs[x_index, y_index].plot(bandit.learning_process)\n",
    "            axs[x_index, y_index].set_title(f'True reward: {bandit.reward}')\n",
    "        plt.show()\n",
    "            \n",
    "    def experiment(self, bandit_rewards, num_trials, path_to_save='', to_save=False):\n",
    "        \"\"\"\n",
    "        The experiment function that runs the experiment with the given bandit rewards and number of trials.\n",
    "        \n",
    "        Args:\n",
    "            bandit_rewards (list): The true rewards list of bandits.\n",
    "            num_trials (int): The number of iterations to run the experiment.\n",
    "            path_to_save (str): The path to save the results.\n",
    "            to_save (bool): Whether to save the results or not.\n",
    "            \n",
    "        \"\"\"\n",
    "        if to_save:    #If save, create the dataframe\n",
    "            self.create_data()\n",
    "        self.bandits = [EpsilonGreedyBandit(reward) for reward in bandit_rewards]    #Initialize the bandit objects in the list\n",
    "        self.rewards = np.zeros(num_trials)    #Create the rewards list\n",
    "        self.regret = []   #Regret list\n",
    "        for i in range(num_trials):    #Run the iterations\n",
    "            if np.random.random() < self.epsilon:    #If the epsilon is enough high, than choose random bandit\n",
    "                index = np.random.randint(len(self.bandits))\n",
    "            else:\n",
    "                index = np.argmax([b.reward_estimate for b in self.bandits])    #If the epsilon is small, then choose the optimal bandit\n",
    "            selected_bandit = self.bandits[index]    #Get the selected bandit\n",
    "            x = selected_bandit.pull()    #Pull a value from the selected bandit\n",
    "            if to_save:    #If save, add the pulled value to the data\n",
    "                new_row = {'Bandit': index, 'Reward': x, 'Algorithm': selected_bandit.__class__.__name__}\n",
    "                self.data = self.data.append(new_row, ignore_index=True)             \n",
    "            self.rewards[i] = x    #Add the result to the rewards array\n",
    "            self.regret.append(selected_bandit.reward - x)    #Get the regret value\n",
    "            selected_bandit.update(x)    #Update the estimated reward for the bandit\n",
    "            self.update_epsilon(i, num_trials)    #Update the epsilon \n",
    "        self.report(path_to_save, to_save)    #Call the report function\n",
    "        return self.rewards\n",
    "    \n",
    "    def report(self, path_to_save, to_save=False):\n",
    "        \"\"\"\n",
    "        The report function that prints the report for the experiment.\n",
    "        \n",
    "        Args:\n",
    "            path_to_save (str): The path to save the results of the experiment.\n",
    "            to_save (bool): Whether to save the results or not.\n",
    "        \n",
    "        \"\"\"\n",
    "        print(' '*45 +'\\033[1m' + 'Report' + '\\033[0m')\n",
    "        if self.bandits is None:     #If no experiment done before, raise an exception\n",
    "            raise Exception('Run the experiment first')\n",
    "             \n",
    "        #Part1 - plot the learning curves \n",
    "        print('\\n-= Part 1 =-')\n",
    "        self.plot1()\n",
    "        \n",
    "        #Part2 - plot the cumulative rewards\n",
    "        print('\\n-= Part 2 =-')\n",
    "        cumulative_rewards = np.cumsum(self.rewards)\n",
    "        plt.plot(cumulative_rewards)\n",
    "        plt.title('Plot of cumulative rewards\\n', size = 15)\n",
    "        plt.show()\n",
    "        \n",
    "        #Part3 - if save, save the data\n",
    "        if to_save:\n",
    "            self.data.to_csv(path_to_save, index=False)\n",
    "            \n",
    "        #Part4 - print the cumulative reward\n",
    "        cumulative_reward = cumulative_rewards[-1]\n",
    "        print(f'The sum reward for the experiment is {cumulative_reward}')\n",
    "        print(f'The cumulative reward for the experiment is {cumulative_rewards}')\n",
    "        \n",
    "        #Part5 - print the cumulative regret\n",
    "        cumulative_regrets = np.cumsum(self.regret)\n",
    "        cumulative_regret = cumulative_regrets[-1]\n",
    "        print(f'\\nThe sum of regret for the experiment is {cumulative_regret}')\n",
    "        print(f'The cumulative regret for the experiment is {cumulative_regrets}')\n",
    "        \n",
    "    def create_data(self):\n",
    "        \"\"\"\n",
    "        Creates the dataset for storing the data.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.data = pd.DataFrame(columns=['Bandit', 'Reward', 'Algorithm'])    #Create the dataframe for storing data\n",
    "        print('The dataframe for storing data is created.')\n",
    "        \n",
    "    def update_epsilon(self, current_trial, num_trials):\n",
    "        \"\"\"\n",
    "        Updates the epsilon based on the current trial and overall number of trials.\n",
    "        \n",
    "        Args:\n",
    "            current_trial: The current trial.\n",
    "            num_trials: The overall number of trials the experiment is going to run.\n",
    "        \"\"\"\n",
    "        self.epsilon = 1 - (current_trial/num_trials)    #Update the epsilon \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_rewards = EpsilonGreedyBandit(Bandit).experiment(bandit_rewards_list, num_trials, './data/EpsilonGreedy.csv', to_save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSamplingBandit(Bandit):\n",
    "    \"\"\" \n",
    "    Represents a epsilon greedy bandit with the following attributes and methods.\n",
    "    \n",
    "    Attributes:\n",
    "        reward (float): The true reward of the bandit.\n",
    "    Methods:\n",
    "        pull(): Pulls a value for the bandit.\n",
    "        sample(): Samples a value from the estimated reward.\n",
    "        update(x: float): Updates the estimated reward of the bandit.\n",
    "        plot1(): Plots the learning process of the bandits.\n",
    "        experiment(bandit_rewards: list, num_trials: int, path_to_save: str, to_save: bool): Run the experiment with given bandits, number of trials and whether to save the results or not.\n",
    "        report(path_to_save: str, to_save: bool): Prints the report of the experiment.\n",
    "        create_data(): Creates the dataframe for saving\n",
    "        update_epsilon(): Updates the epsilon with rate 1/t.\n",
    "    \"\"\"\n",
    "    def __init__(self, reward):\n",
    "        \"\"\"\n",
    "        Initilizes a new instance of ThompsonSamplingBandit class with the following arguments:\n",
    "        \n",
    "        Args:\n",
    "            reward (float): The true reward of the bandit. \n",
    "        \n",
    "        \"\"\"\n",
    "        self.reward = reward    #The true reward\n",
    "        self.m = 0    #The estimated reward\n",
    "        self.lambda_ = 1    #Parameter lambda\n",
    "        self.tau = 1    #Parameter tau\n",
    "        self.N = 0    #Iteration count\n",
    "        self.sum_x = 0    #Cumulative reward\n",
    "        self.learning_process = []    #Learning process list\n",
    "\n",
    "    def pull(self):\n",
    "        \"\"\"\n",
    "        Pulls a value for the bandit based on the true reward (the value is pulled from normal distribution with mean = \"true bandit reward\" and 1 std).\n",
    "        \n",
    "        \"\"\"\n",
    "        return np.random.randn() / np.sqrt(self.tau) + self.reward\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Samples a value for the bandit based on the estimated reward.\n",
    "        \"\"\"\n",
    "        return np.random.randn() / np.sqrt(self.lambda_) + self.m\n",
    "    \n",
    "    def update(self, x):\n",
    "        \"\"\"\n",
    "        Updates the estimated reward for the bandit based on the new, pulled value.\n",
    "        \n",
    "        Args:\n",
    "            x (float): The new value pulled for the bandit, the value to update for the estimated reward.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.lambda_ += self.tau    #Update lambda\n",
    "        self.sum_x+=x    #Update the cumulative sum\n",
    "        self.m = (self.tau * self.sum_x)/self.lambda_    #Update the reward estimate\n",
    "        self.learning_process.append(self.m)    #Learning process update list\n",
    "        self.N += 1    #Increment iteration count\n",
    "\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'A bandit with actual reward: {self.reward} and estimated reward {self.m}'\n",
    "    \n",
    "    def plot1(self):\n",
    "        \"\"\"\n",
    "        Plots the learning curves after the experiment for the existing bandits. Creates a subplot of all the bandits learning curves.\n",
    "        \n",
    "        \"\"\"\n",
    "        fig, axs = plt.subplots(2, 2, figsize = (10, 8))\n",
    "        plt.suptitle('The learning curves of all 4 bandits', size=15)\n",
    "        for index, bandit in enumerate(self.bandits):\n",
    "            x_index = 1 if index>=2 else 0 \n",
    "            y_index = index%2\n",
    "            axs[x_index, y_index].plot(bandit.learning_process)\n",
    "            axs[x_index, y_index].set_title(f'True reward: {bandit.reward}')\n",
    "        plt.show()\n",
    "            \n",
    "    def experiment(self, bandit_rewards, num_trials, path_to_save, to_save=False):\n",
    "        \"\"\"\n",
    "        The experiment function that runs the experiment with the given bandit rewards and number of trials.\n",
    "        \n",
    "        Args:\n",
    "            bandit_rewards (list): The true rewards list of bandits.\n",
    "            num_trials (int): The number of iterations to run the experiment.\n",
    "            path_to_save (str): The path to save the results.\n",
    "            to_save (bool): Whether to save the results or not.\n",
    "            \n",
    "        \"\"\"\n",
    "        if to_save:    #if save create data\n",
    "            self.create_data()\n",
    "        self.bandits = [ThompsonSamplingBandit(reward) for reward in bandit_rewards]    #Initialize bandits\n",
    "        self.rewards = np.zeros(num_trials)    #Create reward array\n",
    "        self.regret = []\n",
    "        for i in range(num_trials):\n",
    "            index = np.argmax([b.sample() for b in self.bandits])    #Sample from bandit rewards\n",
    "            selected_bandit = self.bandits[index]\n",
    "            x = selected_bandit.pull()\n",
    "            if to_save:\n",
    "                new_row = {'Bandit': index, 'Reward': x, 'Algorithm': selected_bandit.__class__.__name__}\n",
    "                self.data = self.data.append(new_row, ignore_index=True)             \n",
    "            self.rewards[i] = x\n",
    "            self.regret.append(selected_bandit.reward - x)\n",
    "            selected_bandit.update(x)\n",
    "        self.report(path_to_save, to_save) \n",
    "        return self.rewards\n",
    "\n",
    "    def report(self, path_to_save, to_save=False):\n",
    "        \"\"\"\n",
    "        The report function that prints the report for the experiment.\n",
    "        \n",
    "        Args:\n",
    "            path_to_save (str): The path to save the results of the experiment.\n",
    "            to_save (bool): Whether to save the results or not.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.bandits is None:\n",
    "            raise Exception('Run the experiment first')\n",
    "        print(' '*35 +'\\033[1m' + 'Report' + '\\033[0m')\n",
    "        #Part1   \n",
    "        self.plot1()\n",
    "        \n",
    "        #Part2\n",
    "        cumulative_rewards = np.cumsum(self.rewards)\n",
    "        plt.plot(cumulative_rewards)\n",
    "        plt.title('Plot of cumulative rewards\\n', size = 15)\n",
    "        plt.show()\n",
    "        \n",
    "        #Part3\n",
    "        if to_save:\n",
    "            self.data.to_csv(path_to_save, index=False)\n",
    "            \n",
    "        #Part4\n",
    "        cumulative_reward = cumulative_rewards[-1]\n",
    "        print(f'The sum reward for the experiment is {cumulative_reward}')\n",
    "        print(f'The cumulative reward for the experiment is {cumulative_rewards}')\n",
    "        \n",
    "        #Part5\n",
    "        cumulative_regrets = np.cumsum(self.regret)\n",
    "        cumulative_regret = cumulative_regrets[-1]\n",
    "        print(f'\\nThe sum of regret for the experiment is {cumulative_regret}')\n",
    "        print(f'The cumulative regret for the experiment is {cumulative_regrets}')\n",
    "\n",
    "    def create_data(self):\n",
    "        \"\"\"\n",
    "        Creates the dataset for storing data.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.data = pd.DataFrame(columns=['Bandit', 'Reward', 'Algorithm'])\n",
    "        print('The dataframe for storing data is created.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816fdb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "thompson_rewards = ThompsonSamplingBandit(Bandit).experiment(bandit_rewards_list, num_trials, './data/ThompsonSampling.csv', to_save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison(eps_reward, thompson_reward, num_trials):   \n",
    "    \"\"\"\n",
    "    A function for comparing Epsilon Greedy with Thompson Sampling algorithms.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        eps_reward (list): Epsilon rewards.\n",
    "        thompson_reward(list): Thompson rewards.\n",
    "        num_trails (int): The number of trials.\n",
    "    \n",
    "    \"\"\"\n",
    "    plt.plot(np.arange(num_trials), np.cumsum(eps_reward), label='Epsilon-Greedy')\n",
    "    plt.plot(np.arange(num_trials), np.cumsum(thompson_reward), label='Thompson Sampling')\n",
    "    plt.xlabel('#Trials')\n",
    "    plt.ylabel('Estimated Reward')\n",
    "    plt.title('Comparison of Epsilon-Greedy and Thompson Sampling')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b9daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison(eps_rewards, thompson_rewards, num_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56445958",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41738cb3",
   "metadata": {},
   "source": [
    "In summary, Thompson Sampling is a probabilistic algorithm that dynamically adjusts its exploration based on the observed rewards, while Epsilon-Greedy is a deterministic algorithm that balances exploration and exploitation based on a fixed epsilon value (or adjusting epsilon value). Thompson Sampling is generally considered more sophisticated and capable of achieving better performance in uncertain environments, while Epsilon-Greedy is simpler and easier to implement but may require careful tuning of the epsilon value for the optimal results.\n",
    "\n",
    "In our experiment, it is visible how Thopson Sampling achieves higher estimated reward through the experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
